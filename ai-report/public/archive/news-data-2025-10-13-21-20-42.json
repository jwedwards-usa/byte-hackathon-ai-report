{
  "mainHeadline": {
    "text": "OpenAI #15: More on OpenAIâ€™s Paranoid Lawfare Against Advocates of SB 53",
    "url": "https://www.lesswrong.com/posts/txTKHL2dCqnC7QsEX/openai-15-more-on-openai-s-paranoid-lawfare-against"
  },
  "topStories": [
    {
      "text": "Salesforce bets on AI 'agents' to fix what it calls a $7 billion problem in enterprise software",
      "url": "https://venturebeat.com/ai/salesforce-bets-on-ai-agents-to-fix-what-it-calls-a-usd7-billion-problem-in"
    },
    {
      "text": "Machine Learning for Detection and Analysis of Novel LLM Jailbreaks",
      "url": "https://arxiv.org/abs/2510.01644"
    },
    {
      "text": "nanochat",
      "url": "https://simonwillison.net/2025/Oct/13/nanochat/#atom-everything"
    }
  ],
  "leftColumn": [
    {
      "text": "Detecting spills using thermal imaging, pretrained deep learning models, and a robotic platform",
      "url": "https://arxiv.org/abs/2510.08770"
    },
    {
      "text": "Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?",
      "url": "https://arxiv.org/abs/2510.06594"
    },
    {
      "text": "Do LLMs Know They Are Being Tested? Evaluation Awareness and Incentive-Sensitive Failures in GPT-OSS-20B",
      "url": "https://arxiv.org/abs/2510.08624"
    },
    {
      "text": "OPENAI PARTNERS WITH BROADCOM TO PRODUCE ITS OWN AI CHIPS",
      "url": "https://www.theverge.com/news/798827/openai-broadcom-custom-ai-chips"
    },
    {
      "text": "OpenAI and Broadcom announce strategic collaboration to deploy 10 gigawatts of OpenAI-designed AI accelerators",
      "url": "https://openai.com/index/openai-and-broadcom-announce-strategic-collaboration"
    },
    {
      "text": "Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training",
      "url": "https://arxiv.org/abs/2507.01752"
    },
    {
      "text": "PATTERN ENHANCED MULTI-TURN JAILBREAKING: EXPLOITING STRUCTURAL VULNERABILITIES IN LARGE LANGUAGE MODELS",
      "url": "https://arxiv.org/abs/2510.08859"
    },
    {
      "text": "Sample Blog Post",
      "url": "https://www.anthropic.com/news"
    }
  ],
  "centerColumn": [
    {
      "text": "The Model's Language Matters: A Comparative Privacy Analysis of LLMs",
      "url": "https://arxiv.org/abs/2510.08813"
    },
    {
      "text": "MOSAIC: Multi-agent Orchestration for Task-Intelligent Scientific Coding",
      "url": "https://arxiv.org/abs/2510.08804"
    },
    {
      "text": "ConPoSe: LLM-Guided Contact Point Selection for Scalable Cooperative Object Pushing",
      "url": "https://arxiv.org/abs/2510.08705"
    },
    {
      "text": "Efficient Autoregressive Inference for Transformer Probabilistic Models",
      "url": "https://arxiv.org/abs/2510.09477"
    },
    {
      "text": "When to Reason: Semantic Router for vLLM",
      "url": "https://arxiv.org/abs/2510.08731"
    },
    {
      "text": "SAFER-AiD: Saccade-Assisted Foveal-peripheral vision Enhanced Reconstruction for Adversarial Defense",
      "url": "https://arxiv.org/abs/2510.08761"
    },
    {
      "text": "Re-Identifying K\\={a}k\\={a} with AI-Automated Video Key Frame Extraction",
      "url": "https://arxiv.org/abs/2510.08775"
    },
    {
      "text": "MSDM: Generating Task-Specific Pathology Images with a Multimodal Conditioned Diffusion Model for Cell and Nuclei Segmentation",
      "url": "https://arxiv.org/abs/2510.09121"
    }
  ],
  "rightColumn": [
    {
      "text": "Large Language Models Do NOT Really Know What They Don't Know",
      "url": "https://arxiv.org/abs/2510.09033"
    },
    {
      "text": "Convergence of optimizers implies eigenvalues filtering at equilibrium",
      "url": "https://arxiv.org/abs/2510.09034"
    },
    {
      "text": "LM Fight Arena: Benchmarking Large Multimodal Models via Game Competition",
      "url": "https://arxiv.org/abs/2510.08928"
    },
    {
      "text": "Improving Anomaly Detection in Industrial Time Series: The Role of Segmentation and Heterogeneous Ensemble",
      "url": "https://arxiv.org/abs/2510.09079"
    },
    {
      "text": "EcphoryRAG: Re-Imagining Knowledge-Graph RAG via Human Associative Memory",
      "url": "https://arxiv.org/abs/2510.08958"
    },
    {
      "text": "GraphGhost: Tracing Structures Behind Large Language Models",
      "url": "https://arxiv.org/abs/2510.08613"
    },
    {
      "text": "Exploring Cross-Lingual Knowledge Transfer via Transliteration-Based MLM Fine-Tuning for Critically Low-resource Chakma Language",
      "url": "https://arxiv.org/abs/2510.09032"
    },
    {
      "text": "Agentic Exploration of Physics Models",
      "url": "https://arxiv.org/abs/2509.24978"
    }
  ],
  "lastUpdated": "2025-10-13T21:20:42Z"
}