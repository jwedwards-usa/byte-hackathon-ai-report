{
  "mainHeadline": {
    "text": "MULTI-TURN JAILBREAKING OF ALIGNED LLMS VIA LEXICAL ANCHOR TREE SEARCH",
    "url": "https://arxiv.org/abs/2601.02670"
  },
  "topStories": [
    {
      "text": "Artificial Analysis overhauls its AI Intelligence Index, replacing popular benchmarks with 'real-world' tests",
      "url": "https://venturebeat.com/technology/artificial-analysis-overhauls-its-ai-intelligence-index-replacing-popular"
    },
    {
      "text": "AI-EXPOSED JOBS DETERIORATED BEFORE CHATGPT",
      "url": "https://arxiv.org/abs/2601.02554"
    },
    {
      "text": "EvoGPT: Leveraging LLM-Driven Seed Diversity to Improve Search-Based Test Suite Generation",
      "url": "https://arxiv.org/abs/2505.12424"
    }
  ],
  "leftColumn": [
    {
      "text": "LLMs contain a LOT of parameters. But what’s a parameter?",
      "url": "https://www.technologyreview.com/2026/01/07/1130795/what-even-is-a-parameter/"
    },
    {
      "text": "CutisAI: Deep Learning Framework for Automated Dermatology and Cancer Screening",
      "url": "https://arxiv.org/abs/2601.02562"
    },
    {
      "text": "How hard is it to inoculate against misalignment generalization?",
      "url": "https://www.alignmentforum.org/posts/G4YXXbKt5cNSQbjXM/how-hard-is-it-to-inoculate-against-misalignment"
    },
    {
      "text": "NorwAI's Large Language Models: Technical Report",
      "url": "https://arxiv.org/abs/2601.03034"
    },
    {
      "text": "Sample Blog Post",
      "url": "https://www.anthropic.com/news"
    },
    {
      "text": "Why Supply Chain is the Best Domain for Data Scientists in 2026 (And How to Learn It)",
      "url": "https://towardsdatascience.com/why-supply-chain-is-the-best-domain-for-data-scientists-in-2026-and-how-to-learn-it/"
    },
    {
      "text": "Accelerating Storage-Based Training for Graph Neural Networks",
      "url": "https://arxiv.org/abs/2601.01473"
    },
    {
      "text": "SaVe-TAG: LLM-based Interpolation for Long-Tailed Text-Attributed Graphs",
      "url": "https://arxiv.org/abs/2410.16882"
    }
  ],
  "centerColumn": [
    {
      "text": "From Muscle to Text with MyoText: sEMG to Text via Finger Classification and Transformer-Based Decoding",
      "url": "https://arxiv.org/abs/2601.03098"
    },
    {
      "text": "The performances of the Chinese and U.S. Large Language Models on the Topic of Chinese Culture",
      "url": "https://arxiv.org/abs/2601.02830"
    },
    {
      "text": "Punctuation-aware Hybrid Trainable Sparse Attention for Large Language Models",
      "url": "https://arxiv.org/abs/2601.02819"
    },
    {
      "text": "FLUID: Training-Free Face De-identification via Latent Identity Substitution",
      "url": "https://arxiv.org/abs/2511.17005"
    },
    {
      "text": "LongDA: Benchmarking LLM Agents for Long-Document Data Analysis",
      "url": "https://arxiv.org/abs/2601.02598"
    },
    {
      "text": "RadioDiff-Flux: Efficient Radio Map Construction via Generative Denoise Diffusion Model Trajectory Midpoint Reuse",
      "url": "https://arxiv.org/abs/2601.02790"
    },
    {
      "text": "TAAF: A Trace Abstraction and Analysis Framework Synergizing Knowledge Graphs and LLMs",
      "url": "https://arxiv.org/abs/2601.02632"
    },
    {
      "text": "Electricity Price Forecasting: Bridging Linear Models, Neural Networks and Online Learning",
      "url": "https://arxiv.org/abs/2601.02856"
    }
  ],
  "rightColumn": [
    {
      "text": "Self-Supervised Masked Autoencoders with Dense-Unet for Coronary Calcium Removal in limited CT Data",
      "url": "https://arxiv.org/abs/2601.02392"
    },
    {
      "text": "Fast Conformal Prediction using Conditional Interquantile Intervals",
      "url": "https://arxiv.org/abs/2601.02769"
    },
    {
      "text": "Detecting Hallucinations in Retrieval-Augmented Generation via Semantic-level Internal Reasoning Graph",
      "url": "https://arxiv.org/abs/2601.03052"
    },
    {
      "text": "Statistical Inference for Fuzzy Clustering",
      "url": "https://arxiv.org/abs/2601.02656"
    },
    {
      "text": "SWaRL: Safeguard Code Watermarking via Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.02602"
    },
    {
      "text": "Compressed code: the hidden effects of quantization and distillation on programming tokens",
      "url": "https://arxiv.org/abs/2601.02563"
    },
    {
      "text": "VisRet: Visualization Improves Knowledge-Intensive Text-to-Image Retrieval",
      "url": "https://arxiv.org/abs/2505.20291"
    },
    {
      "text": "Large Language Models can Achieve Social Balance",
      "url": "https://arxiv.org/abs/2410.04054"
    }
  ],
  "lastUpdated": "2026-01-07T12:58:48Z"
}