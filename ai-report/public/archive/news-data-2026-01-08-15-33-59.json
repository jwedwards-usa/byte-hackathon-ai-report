{
  "mainHeadline": {
    "text": "AI #150: While Claude Codes",
    "url": "https://www.lesswrong.com/posts/fWJsqHXHBAEd8rq69/ai-150-while-claude-codes"
  },
  "topStories": [
    {
      "text": "OpenAI launches ChatGPT Health, encouraging users to connect their medical records",
      "url": "https://www.theverge.com/ai-artificial-intelligence/857640/openai-launches-chatgpt-health-connect-medical-records"
    },
    {
      "text": "Can We Make AI Alignment Framing Less Wrong?",
      "url": "https://www.lesswrong.com/posts/gapGhQxqYQrayDdLd/can-we-make-ai-alignment-framing-less-wrong"
    },
    {
      "text": "The AI Infrastructure Security Shortlist",
      "url": "https://www.lesswrong.com/posts/xkE4zEzmArxgskZ96/the-ai-infrastructure-security-shortlist"
    }
  ],
  "leftColumn": [
    {
      "text": "Nous Research's NousCoder-14B is an open-source coding model landing right in the Claude Code moment",
      "url": "https://venturebeat.com/technology/nous-researchs-nouscoder-14b-is-an-open-source-coding-model-landing-right-in"
    },
    {
      "text": "OPENAI GPT-5 SYSTEM CARD",
      "url": "https://arxiv.org/abs/2601.03267"
    },
    {
      "text": "JAILBREAKING LLMS WITHOUT GRADIENTS OR PRIORS: EFFECTIVE AND TRANSFERABLE ATTACKS",
      "url": "https://arxiv.org/abs/2601.03420"
    },
    {
      "text": "EvoGPT: Leveraging LLM-Driven Seed Diversity to Improve Search-Based Test Suite Generation",
      "url": "https://arxiv.org/abs/2505.12424"
    },
    {
      "text": "AI-EXPOSED JOBS DETERIORATED BEFORE CHATGPT",
      "url": "https://arxiv.org/abs/2601.02554"
    },
    {
      "text": "JAILBREAKING COMMERCIAL BLACK-BOX LLMS WITH EXPLICITLY HARMFUL PROMPTS",
      "url": "https://arxiv.org/abs/2508.10390"
    },
    {
      "text": "A Comparative Study of Traditional Machine Learning, Deep Learning, and Large Language Models for Mental Health Forecasting using Smartphone Sensing Data",
      "url": "https://arxiv.org/abs/2601.03603"
    },
    {
      "text": "GuardEval: A Multi-Perspective Benchmark for Evaluating Safety, Fairness, and Robustness in LLM Moderators",
      "url": "https://arxiv.org/abs/2601.03273"
    }
  ],
  "centerColumn": [
    {
      "text": "Batch-of-Thought: Cross-Instance Learning for Enhanced LLM Reasoning",
      "url": "https://arxiv.org/abs/2601.02950"
    },
    {
      "text": "BREAKING SELF-ATTENTION FAILURE: RETHINKING QUERY INITIALIZATION FOR INFRARED SMALL TARGET DETECTION",
      "url": "https://arxiv.org/abs/2601.02837"
    },
    {
      "text": "Explainable Fuzzy GNNs for Leak Detection in Water Distribution Networks",
      "url": "https://arxiv.org/abs/2601.03062"
    },
    {
      "text": "Whose Facts Win? LLM Source Preferences under Knowledge Conflicts",
      "url": "https://arxiv.org/abs/2601.03746"
    },
    {
      "text": "Do LLM Self-Explanations Help Users Predict Model Behavior? Evaluating Counterfactual Simulatability with Pragmatic Perturbations",
      "url": "https://arxiv.org/abs/2601.03775"
    },
    {
      "text": "TAAF: A Trace Abstraction and Analysis Framework Synergizing Knowledge Graphs and LLMs",
      "url": "https://arxiv.org/abs/2601.02632"
    },
    {
      "text": "Improved Evidence Extraction for Document Inconsistency Detection with LLMs",
      "url": "https://arxiv.org/abs/2601.02627"
    },
    {
      "text": "LongDA: Benchmarking LLM Agents for Long-Document Data Analysis",
      "url": "https://arxiv.org/abs/2601.02598"
    }
  ],
  "rightColumn": [
    {
      "text": "PiDR: Physics-Informed Inertial Dead Reckoning for Autonomous Platforms",
      "url": "https://arxiv.org/abs/2601.03040"
    },
    {
      "text": "Stable Language Guidance for Vision-Language-Action Models",
      "url": "https://arxiv.org/abs/2601.04052"
    },
    {
      "text": "Topology-Independent Robustness of the Weighted Mean under Label Poisoning Attacks in Heterogeneous Decentralized Learning",
      "url": "https://arxiv.org/abs/2601.02682"
    },
    {
      "text": "IntroLM: Introspective Language Models via Prefilling-Time Self-Evaluation",
      "url": "https://arxiv.org/abs/2601.03511"
    },
    {
      "text": "TimeDistill: Efficient Long-Term Time Series Forecasting with MLP via Cross-Architecture Distillation",
      "url": "https://arxiv.org/abs/2502.15016"
    },
    {
      "text": "Adversarial Question Answering Robustness: A Multi-Level Error Analysis and Mitigation Study",
      "url": "https://arxiv.org/abs/2601.02700"
    },
    {
      "text": "A Systematic Comparison between Extractive Self-Explanations and Human Rationales in Text Classification",
      "url": "https://arxiv.org/abs/2410.03296"
    },
    {
      "text": "Beyond Perplexity: A Lightweight Benchmark for Knowledge Retention in Supervised Fine-Tuning",
      "url": "https://arxiv.org/abs/2601.03505"
    }
  ],
  "lastUpdated": "2026-01-08T15:33:59Z"
}