{
  "mainHeadline": {
    "text": "AI #151: While Claude Coworks",
    "url": "https://www.lesswrong.com/posts/L27yM3qBqDnigtxLM/ai-151-while-claude-coworks"
  },
  "topStories": [
    {
      "text": "OPENAI’S CHATGPT TRANSLATOR CHALLENGES GOOGLE TRANSLATE",
      "url": "https://www.theverge.com/news/862448/openai-chatgpt-translate-tool-launch-website"
    },
    {
      "text": "AN OPENAI SAFETY RESEARCH LEAD DEPARTED FOR ANTHROPIC",
      "url": "https://www.theverge.com/ai-artificial-intelligence/862402/openai-safety-lead-model-policy-departs-for-anthropic-alignment-andrea-vallone"
    },
    {
      "text": "Reflections on TA-ing Harvard’s first AI safety course",
      "url": "https://www.lesswrong.com/posts/gcFB2RT5vpKHbH4ic/reflections-on-ta-ing-harvard-s-first-ai-safety-course"
    }
  ],
  "leftColumn": [
    {
      "text": "Comparative Assessment of Concrete Compressive Strength Prediction at Industry Scale Using Embedding-based Neural Networks, Transformers, and Traditional Machine Learning Approaches",
      "url": "https://arxiv.org/abs/2601.09096"
    },
    {
      "text": "A Benchmark for End-to-End Zero-Shot Biomedical Relation Extraction with LLMs: Experiments with OpenAI Models",
      "url": "https://arxiv.org/abs/2504.04083"
    },
    {
      "text": "BREAKING THROUGH AI’S MEMORY WALL WITH TOKEN WAREHOUSING",
      "url": "https://venturebeat.com/infrastructure/breaking-through-ais-memory-wall-with-token-warehousing"
    },
    {
      "text": "Z.ai's open source GLM-Image beats Google's Nano Banana Pro at complex text rendering, but not aesthetics",
      "url": "https://venturebeat.com/technology/z-ais-open-source-glm-image-beats-googles-nano-banana-pro-at-complex-text"
    },
    {
      "text": "OPENAI PARTNERS WITH CEREBRAS  ",
      "url": "https://openai.com/index/cerebras-partnership"
    },
    {
      "text": "Test your interpretability techniques by de-censoring Chinese models",
      "url": "https://www.lesswrong.com/posts/7gp76q4rWLFi6sFqm/test-your-interpretability-techniques-by-de-censoring-1"
    },
    {
      "text": "Can AI Dream of Unseen Galaxies? Conditional Diffusion Model for Galaxy Morphology Augmentation",
      "url": "https://arxiv.org/abs/2506.16233"
    },
    {
      "text": "Training Large Neural Networks With Low-Dimensional Error Feedback",
      "url": "https://arxiv.org/abs/2502.20580"
    }
  ],
  "centerColumn": [
    {
      "text": "ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection",
      "url": "https://arxiv.org/abs/2601.09195"
    },
    {
      "text": "Creating a Hybrid Rule and Neural Network Based Semantic Tagger using Silver Standard Data: the PyMUSAS framework for Multilingual Semantic Annotation",
      "url": "https://arxiv.org/abs/2601.09648"
    },
    {
      "text": "AgenticIE: An Adaptive Agent for Information Extraction from Complex Regulatory Documents",
      "url": "https://arxiv.org/abs/2509.11773"
    },
    {
      "text": "Enabling Global, Human-Centered Explanations for LLMs:From Tokens to Interpretable Code and Test Generation",
      "url": "https://arxiv.org/abs/2503.16771"
    },
    {
      "text": "Universal Dynamics of Warmup Stable Decay: understanding WSD beyond Transformers",
      "url": "https://arxiv.org/abs/2601.09000"
    },
    {
      "text": "Boltzmann Tulpas",
      "url": "https://www.lesswrong.com/posts/gSdhh33y9kYWQnCzD/boltzmann-tulpas"
    },
    {
      "text": "Uncertainty in Machine Learning: Probability \u0026 Noise",
      "url": "https://machinelearningmastery.com/uncertainty-in-machine-learning-probability-noise/"
    },
    {
      "text": "Announcing the winner of the Global AI Film Award",
      "url": "https://blog.google/company-news/inside-google/around-the-globe/google-middle-east/winner-of-the-global-ai-film-award/"
    }
  ],
  "rightColumn": [
    {
      "text": "Optimism Without Regularization: Constant Regret in Zero-Sum Games",
      "url": "https://arxiv.org/abs/2506.16736"
    },
    {
      "text": "Structured yet Bounded Temporal Understanding in Large Language Models",
      "url": "https://arxiv.org/abs/2510.16685"
    },
    {
      "text": "Towards a Self-Driving Trigger at the LHC: Adaptive Response in Real Time",
      "url": "https://arxiv.org/abs/2601.08910"
    },
    {
      "text": "LaoBench: A Large-Scale Multidimensional Lao Benchmark for Large Language Models",
      "url": "https://arxiv.org/abs/2511.11334"
    },
    {
      "text": "BalDRO: A Distributionally Robust Optimization based Framework for Large Language Model Unlearning",
      "url": "https://arxiv.org/abs/2601.09172"
    },
    {
      "text": "$D^2Prune$: Sparsifying Large Language Models via Dual Taylor Expansion and Attention Distribution Awareness",
      "url": "https://arxiv.org/abs/2601.09176"
    },
    {
      "text": "From Hawkes Processes to Attention: Time-Modulated Mechanisms for Event Sequences",
      "url": "https://arxiv.org/abs/2601.09220"
    },
    {
      "text": "XLinear: A Lightweight and Accurate MLP-Based Model for Long-Term Time Series Forecasting with Exogenous Inputs",
      "url": "https://arxiv.org/abs/2601.09237"
    }
  ],
  "lastUpdated": "2026-01-16T02:07:44Z"
}