{
  "mainHeadline": {
    "text": "From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks",
    "url": "https://arxiv.org/abs/2405.15164"
  },
  "topStories": [
    {
      "text": "The LLM Has Left The Chat: Evidence of Bail Preferences in Large Language Models",
      "url": "https://www.lesswrong.com/posts/6JdSJ63LZ4TuT5cTH/the-llm-has-left-the-chat-evidence-of-bail-preferences-in"
    },
    {
      "text": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling",
      "url": "https://arxiv.org/abs/2402.12226"
    },
    {
      "text": "Translating Federated Learning Algorithms in Python into CSP Processes Using ChatGPT",
      "url": "https://arxiv.org/abs/2506.07173"
    }
  ],
  "leftColumn": [
    {
      "text": "Memristor-Based Neural Network Accelerators for Space Applications: Enhancing Performance with Temporal Averaging and SIRENs",
      "url": "https://arxiv.org/abs/2509.04506"
    },
    {
      "text": "GPT-5 Thinking in ChatGPT (aka Research Goblin) is shockingly good at search",
      "url": "https://simonwillison.net/2025/Sep/6/research-goblin/#atom-everything"
    },
    {
      "text": "Can Multiple Responses from an LLM Reveal the Sources of Its Uncertainty?",
      "url": "https://arxiv.org/abs/2509.04464"
    },
    {
      "text": "ASCENDgpt: A Phenotype-Aware Transformer Model for Cardiovascular Risk Prediction from Electronic Health Records",
      "url": "https://arxiv.org/abs/2509.04485"
    },
    {
      "text": "Uncertain but Useful: Leveraging CNN Variability into Data Augmentation",
      "url": "https://arxiv.org/abs/2509.05238"
    },
    {
      "text": "HuggingGraph: Understanding the Supply Chain of LLM Ecosystem",
      "url": "https://arxiv.org/abs/2507.14240"
    },
    {
      "text": "Graph Transformer-Based Flood Susceptibility Mapping: Application to the French Riviera and Railway Infrastructure Under Climate Change",
      "url": "https://arxiv.org/abs/2504.03727"
    },
    {
      "text": "From Silent Signals to Natural Language: A Dual-Stage Transformer-LLM Approach",
      "url": "https://arxiv.org/abs/2509.04507"
    }
  ],
  "centerColumn": [
    {
      "text": "Learning Counterfactually Fair Models via Improved Generation with Neural Causal Models",
      "url": "https://arxiv.org/abs/2502.12796"
    },
    {
      "text": "Don't Trade Off Safety: Diffusion Regularization for Constrained Offline RL",
      "url": "https://arxiv.org/abs/2502.12391"
    },
    {
      "text": "Robust Experts: the Effect of Adversarial Training on CNNs with Sparse Mixture-of-Experts Layers",
      "url": "https://arxiv.org/abs/2509.05086"
    },
    {
      "text": "Caution for the Environment: Multimodal LLM Agents are Susceptible to Environmental Distractions",
      "url": "https://arxiv.org/abs/2408.02544"
    },
    {
      "text": "Q-SafeML: Safety Assessment of Quantum Machine Learning via Quantum Distance Metrics",
      "url": "https://arxiv.org/abs/2509.04536"
    },
    {
      "text": "Dynamical Learning in Deep Asymmetric Recurrent Neural Networks",
      "url": "https://arxiv.org/abs/2509.05041"
    },
    {
      "text": "ViClaim: A Multilingual Multilabel Dataset for Automatic Claim Detection in Videos",
      "url": "https://arxiv.org/abs/2504.12882"
    },
    {
      "text": "Text2Cypher Across Languages: Evaluating and Finetuning LLMs",
      "url": "https://arxiv.org/abs/2506.21445"
    }
  ],
  "rightColumn": [
    {
      "text": "Context Engineering for Trustworthiness: Rescorla Wagner Steering Under Mixed and Inappropriate Contexts",
      "url": "https://arxiv.org/abs/2509.04500"
    },
    {
      "text": "PLaMo 2 Technical Report",
      "url": "https://arxiv.org/abs/2509.04897"
    },
    {
      "text": "L1RA: Dynamic Rank Assignment in LoRA Fine-Tuning",
      "url": "https://arxiv.org/abs/2509.04884"
    },
    {
      "text": "VaccineRAG: Boosting Multimodal Large Language Models' Immunity to Harmful RAG Samples",
      "url": "https://arxiv.org/abs/2509.04502"
    },
    {
      "text": "Memorization $\\neq$ Understanding: Do Large Language Models Have the Ability of Scenario Cognition?",
      "url": "https://arxiv.org/abs/2509.04866"
    },
    {
      "text": "Beyond Ordinary Lipschitz Constraints: Differentially Private Stochastic Optimization with Tsybakov Noise Condition",
      "url": "https://arxiv.org/abs/2509.04668"
    },
    {
      "text": "Behavioral Fingerprinting of Large Language Models",
      "url": "https://arxiv.org/abs/2509.04504"
    },
    {
      "text": "INSEva: A Comprehensive Chinese Benchmark for Large Language Models in Insurance",
      "url": "https://arxiv.org/abs/2509.04455"
    }
  ],
  "lastUpdated": "2025-09-08T06:35:22Z"
}