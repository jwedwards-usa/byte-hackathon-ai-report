{
  "mainHeadline": {
    "text": "AI #151: While Claude Coworks",
    "url": "https://www.lesswrong.com/posts/L27yM3qBqDnigtxLM/ai-151-while-claude-coworks"
  },
  "topStories": [
    {
      "text": "AN OPENAI SAFETY RESEARCH LEAD DEPARTED FOR ANTHROPIC",
      "url": "https://www.theverge.com/ai-artificial-intelligence/862402/openai-safety-lead-model-policy-departs-for-anthropic-alignment-andrea-vallone"
    },
    {
      "text": "Reflections on TA-ing Harvard’s first AI safety course",
      "url": "https://www.lesswrong.com/posts/gcFB2RT5vpKHbH4ic/reflections-on-ta-ing-harvard-s-first-ai-safety-course"
    },
    {
      "text": "OPENAI’S CHATGPT TRANSLATOR CHALLENGES GOOGLE TRANSLATE",
      "url": "https://www.theverge.com/news/862448/openai-chatgpt-translate-tool-launch-website"
    }
  ],
  "leftColumn": [
    {
      "text": "Comparative Assessment of Concrete Compressive Strength Prediction at Industry Scale Using Embedding-based Neural Networks, Transformers, and Traditional Machine Learning Approaches",
      "url": "https://arxiv.org/abs/2601.09096"
    },
    {
      "text": "Test your interpretability techniques by de-censoring Chinese models",
      "url": "https://www.lesswrong.com/posts/7gp76q4rWLFi6sFqm/test-your-interpretability-techniques-by-de-censoring-1"
    },
    {
      "text": "Z.ai's open source GLM-Image beats Google's Nano Banana Pro at complex text rendering, but not aesthetics",
      "url": "https://venturebeat.com/technology/z-ais-open-source-glm-image-beats-googles-nano-banana-pro-at-complex-text"
    },
    {
      "text": "EXCLUSIVE EBOOK: HOW AGI BECAME A CONSEQUENTIAL CONSPIRACY THEORY",
      "url": "https://www.technologyreview.com/2026/01/15/1131079/exclusive-ebook-how-agi-became-a-consequential-conspiracy-theory/"
    },
    {
      "text": "BREAKING THROUGH AI’S MEMORY WALL WITH TOKEN WAREHOUSING",
      "url": "https://venturebeat.com/infrastructure/breaking-through-ais-memory-wall-with-token-warehousing"
    },
    {
      "text": "A Benchmark for End-to-End Zero-Shot Biomedical Relation Extraction with LLMs: Experiments with OpenAI Models",
      "url": "https://arxiv.org/abs/2504.04083"
    },
    {
      "text": "OPENAI PARTNERS WITH CEREBRAS  ",
      "url": "https://openai.com/index/cerebras-partnership"
    },
    {
      "text": "This new, dead simple prompt technique boosts accuracy on LLMs by up to 76% on non-reasoning tasks",
      "url": "https://venturebeat.com/orchestration/this-new-dead-simple-prompt-technique-boosts-accuracy-on-llms-by-up-to-76-on",
      "image": {
        "src": "https://images.ctfassets.net/jdtwqhzvc2n1/6PTJl3Wssuvl5m3nljR103/21ab6a44da755c1cf2dca3b8fdd4ad08/cool-guys.png?w=300\u0026q=30",
        "alt": "This new, dead simple prompt technique boosts accuracy on LLMs by up to 76% on non-reasoning tasks",
        "width": 600,
        "height": 400
      }
    }
  ],
  "centerColumn": [
    {
      "text": "Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion",
      "url": "https://arxiv.org/abs/2512.20249"
    },
    {
      "text": "Informed Consent for AI Consciousness Research: A Talmudic Framework for Graduated Protections",
      "url": "https://arxiv.org/abs/2601.08864"
    },
    {
      "text": "Recidivism and Peer Influence with LLM Text Embeddings in Low Security Correctional Facilities",
      "url": "https://arxiv.org/abs/2509.20634"
    },
    {
      "text": "Differential Privacy for Transformer Embeddings of Text with Nonparametric Variational Information Bottleneck",
      "url": "https://arxiv.org/abs/2601.02307"
    },
    {
      "text": "Enabling Global, Human-Centered Explanations for LLMs:From Tokens to Interpretable Code and Test Generation",
      "url": "https://arxiv.org/abs/2503.16771"
    },
    {
      "text": "Bridging the Gap: Empowering Small Models in Reliable OpenACC-based Parallelization via GEPA-Optimized Prompting",
      "url": "https://arxiv.org/abs/2601.08884"
    },
    {
      "text": "GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models",
      "url": "https://arxiv.org/abs/2601.07632"
    },
    {
      "text": "TeleMem: Building Long-Term and Multimodal Memory for Agentic AI",
      "url": "https://arxiv.org/abs/2601.06037"
    }
  ],
  "rightColumn": [
    {
      "text": "Enhancing Large Language Models for Time-Series Forecasting via Vector-Injected In-Context Learning",
      "url": "https://arxiv.org/abs/2601.07903"
    },
    {
      "text": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head",
      "url": "https://arxiv.org/abs/2601.07832"
    },
    {
      "text": "Do Sparse Autoencoders Identify Reasoning Features in Language Models?",
      "url": "https://arxiv.org/abs/2601.05679"
    },
    {
      "text": "Why are there many equally good models? An Anatomy of the Rashomon Effect",
      "url": "https://arxiv.org/abs/2601.06730"
    },
    {
      "text": "Adaptive Requesting in Decentralized Edge Networks via Non-Stationary Bandits",
      "url": "https://arxiv.org/abs/2601.08760"
    },
    {
      "text": "Beyond One-Size-Fits-All: A Survey of Personalized Affective Computing in Human-Agent Interaction",
      "url": "https://arxiv.org/abs/2304.00377"
    },
    {
      "text": "SpikeVAEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-VAE and Versatile Diffusion",
      "url": "https://arxiv.org/abs/2601.09213"
    },
    {
      "text": "ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation",
      "url": "https://arxiv.org/abs/2601.09703"
    }
  ],
  "lastUpdated": "2026-01-15T18:46:01Z"
}