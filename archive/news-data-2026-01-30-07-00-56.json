{
  "mainHeadline": {
    "text": "Retiring GPT-4o, GPT-4.1, GPT-4.1 mini, and OpenAI o4-mini in ChatGPT",
    "url": "https://openai.com/index/retiring-gpt-4o-and-older-models"
  },
  "topStories": [
    {
      "text": "An explainable vision transformer with transfer learning based efficient drought stress identification",
      "url": "https://arxiv.org/abs/2407.21666"
    },
    {
      "text": "Evaluating ChatGPT on Medical Information Extraction Tasks: Performance, Explainability and Beyond",
      "url": "https://arxiv.org/abs/2601.21767"
    },
    {
      "text": "AI models that simulate internal debate dramatically improve accuracy on complex tasks",
      "url": "https://venturebeat.com/orchestration/ai-models-that-simulate-internal-debate-dramatically-improve-accuracy-on"
    }
  ],
  "leftColumn": [
    {
      "text": "DBELLQUANT: BREAKING THE BELL WITH DOUBLE-BELL TRANSFORMATION FOR LLMS POST TRAINING BINARIZATION",
      "url": "https://arxiv.org/abs/2507.01027"
    },
    {
      "text": "Bridging On-Device and Cloud LLMs for Collaborative Reasoning: A Unified Methodology for Local Routing and Post-Training",
      "url": "https://arxiv.org/abs/2509.24050"
    },
    {
      "text": "Learn-to-Distance: Distance Learning for Detecting LLM-Generated Text",
      "url": "https://arxiv.org/abs/2601.21895"
    },
    {
      "text": "Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in $\\{\\pm 1, \\pm i\\}$",
      "url": "https://arxiv.org/abs/2512.02901"
    },
    {
      "text": "A Unified XAI-LLM Approach for EndotrachealSuctioning Activity Recognition",
      "url": "https://arxiv.org/abs/2601.21802"
    },
    {
      "text": "Are We in a Continual Learning Overhang?",
      "url": "https://www.lesswrong.com/posts/Lby4gMvKcLPoozHfg/are-we-in-a-continual-learning-overhang-1"
    },
    {
      "text": "Taisei Corporation shapes the next generation of talent with ChatGPT",
      "url": "https://openai.com/index/taisei"
    },
    {
      "text": "Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening",
      "url": "https://arxiv.org/abs/2601.21590"
    }
  ],
  "centerColumn": [
    {
      "text": "Finetune-Informed Pretraining Boosts Downstream Performance",
      "url": "https://arxiv.org/abs/2601.20884"
    },
    {
      "text": "ETS: Energy-Guided Test-Time Scaling for Training-Free RL Alignment",
      "url": "https://arxiv.org/abs/2601.21484"
    },
    {
      "text": "The neural networks with tensor weights and emergent fermionic Wick rules in the large-width limit",
      "url": "https://arxiv.org/abs/2507.05303"
    },
    {
      "text": "CleanSurvival: Automated data preprocessing for time-to-event models using reinforcement learning",
      "url": "https://arxiv.org/abs/2502.03946"
    },
    {
      "text": "The Path of Least Resistance: Guiding LLM Reasining Trajectories with Prefix Consensus",
      "url": "https://arxiv.org/abs/2601.21494"
    },
    {
      "text": "Temporal Sepsis Modeling: a Fully Interpretable Relational Way",
      "url": "https://arxiv.org/abs/2601.21747"
    },
    {
      "text": "Revisiting Diffusion Model Predictions Through Dimensionality",
      "url": "https://arxiv.org/abs/2601.21419"
    },
    {
      "text": "Learning to Optimize Job Shop Scheduling Under Structural Uncertainty",
      "url": "https://arxiv.org/abs/2601.21389"
    }
  ],
  "rightColumn": [
    {
      "text": "Gauge-invariant representation holonomy",
      "url": "https://arxiv.org/abs/2601.21653"
    },
    {
      "text": "TabClustPFN: A Prior-Fitted Network for Tabular Data Clustering",
      "url": "https://arxiv.org/abs/2601.21656"
    },
    {
      "text": "Representation Unlearning: Forgetting through Information Compression",
      "url": "https://arxiv.org/abs/2601.21564"
    },
    {
      "text": "Multi-Modal Time Series Prediction via Mixture of Modulated Experts",
      "url": "https://arxiv.org/abs/2601.21547"
    },
    {
      "text": "Expected Return Causes Outcome-Level Mode Collapse in Reinforcement Learning and How to Fix It with Inverse Probability Scaling",
      "url": "https://arxiv.org/abs/2601.21669"
    },
    {
      "text": "Explicit Credit Assignment through Local Rewards and Dependence Graphs in Multi-Agent Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.21523"
    },
    {
      "text": "PPI-SVRG: Unifying Prediction-Powered Inference and Variance Reduction for Semi-Supervised Optimization",
      "url": "https://arxiv.org/abs/2601.21470"
    },
    {
      "text": "Don't be so Stief! Learning KV Cache low-rank approximation over the Stiefel manifold",
      "url": "https://arxiv.org/abs/2601.21686"
    }
  ],
  "lastUpdated": "2026-01-30T07:00:56Z"
}