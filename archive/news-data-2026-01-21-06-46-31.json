{
  "mainHeadline": {
    "text": "Deep learning as program synthesis",
    "url": "https://www.lesswrong.com/posts/Dw8mskAvBX37MxvXo/deep-learning-as-program-synthesis-1"
  },
  "topStories": [
    {
      "text": "Pretraining on Aligned AI Data Dramatically Reduces Misalignmentâ€”Even After Post-Training",
      "url": "https://www.alignmentforum.org/posts/ZeWewFEefCtx4Rj3G/pretraining-on-aligned-ai-data-dramatically-reduces"
    },
    {
      "text": "Predictive Prototyping: Evaluating Design Concepts with ChatGPT",
      "url": "https://arxiv.org/abs/2601.12276"
    },
    {
      "text": "Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training",
      "url": "https://arxiv.org/abs/2508.14904"
    }
  ],
  "leftColumn": [
    {
      "text": "Leveraging ChatGPT and Other NLP Methods for Identifying Risk and Protective Behaviors in MSM: Social Media and Dating apps Text Analysis",
      "url": "https://arxiv.org/abs/2601.13558"
    },
    {
      "text": "Early Prediction of Type 2 Diabetes Using Multimodal data and Tabular Transformers",
      "url": "https://arxiv.org/abs/2601.12981"
    },
    {
      "text": "LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems",
      "url": "https://arxiv.org/abs/2601.14053"
    },
    {
      "text": "A New Strategy for Artificial Intelligence: Training Foundation Models Directly on Human Brain Data",
      "url": "https://arxiv.org/abs/2601.12053"
    },
    {
      "text": "SOCKPUPPETTING: JAILBREAKING LLMS WITHOUT OPTIMIZATION THROUGH OUTPUT PREFIX INJECTION",
      "url": "https://arxiv.org/abs/2601.13359"
    },
    {
      "text": "Who Should Have Surgery? A Comparative Study of GenAI vs Supervised ML for CRS Surgical Outcome Prediction",
      "url": "https://arxiv.org/abs/2601.13710"
    },
    {
      "text": "AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and Conversational Scambaiting by Leveraging LLMs and Federated Learning",
      "url": "https://arxiv.org/abs/2509.05362"
    },
    {
      "text": "CHATGPT SELF PORTRAIT",
      "url": "https://www.lesswrong.com/posts/eg6GgEq6KWPJZQQYE/chatgpt-self-portrait"
    }
  ],
  "centerColumn": [
    {
      "text": "Beyond Known Facts: Generating Unseen Temporal Knowledge to Address Data Contamination in LLM Evaluation",
      "url": "https://arxiv.org/abs/2601.13658"
    },
    {
      "text": "Hardware-Aware Reformulation of Convolutions for Efficient Execution on Specialized AI Hardware: A Case Study on NVIDIA Tensor Cores",
      "url": "https://arxiv.org/abs/2601.11608"
    },
    {
      "text": "VectorLiteRAG: Latency-Aware and Fine-Grained Resource Partitioning for Efficient RAG",
      "url": "https://arxiv.org/abs/2504.08930"
    },
    {
      "text": "Optimizing Energy and Data Collection in UAV-aided IoT Networks using Attention-based Multi-Objective Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.14092"
    },
    {
      "text": "Compton Form Factor Extraction using Quantum Deep Neural Networks",
      "url": "https://arxiv.org/abs/2504.15458"
    },
    {
      "text": "Distance Is All You Need: Radial Dispersion for Uncertainty Estimation in Large Language Models",
      "url": "https://arxiv.org/abs/2512.04351"
    },
    {
      "text": "GazeFormer-MoE: Context-Aware Gaze Estimation via CLIP and MoE Transformer",
      "url": "https://arxiv.org/abs/2601.12316"
    },
    {
      "text": "WISP: Waste- and Interference-Suppressed Distributed Speculative LLM Serving at the Edge via Dynamic Drafting and SLO-Aware Batching",
      "url": "https://arxiv.org/abs/2601.11652"
    }
  ],
  "rightColumn": [
    {
      "text": "Fisher-Orthogonal Projected Natural Gradient Descent for Continual Learning",
      "url": "https://arxiv.org/abs/2601.12816"
    },
    {
      "text": "Can Deep Research Agents Find and Organize? Evaluating the Synthesis Gap with Expert Taxonomies",
      "url": "https://arxiv.org/abs/2601.12369"
    },
    {
      "text": "Cooperative Multi-agent RL with Communication Constraints",
      "url": "https://arxiv.org/abs/2601.12518"
    },
    {
      "text": "Gradient-based Active Learning with Gaussian Processes for Global Sensitivity Analysis",
      "url": "https://arxiv.org/abs/2601.11790"
    },
    {
      "text": "Approximating splits for decision trees quickly in sparse data streams",
      "url": "https://arxiv.org/abs/2601.12525"
    },
    {
      "text": "Shuttling Compiler for Trapped-Ion Quantum Computers Based on Large Language Models",
      "url": "https://arxiv.org/abs/2512.18021"
    },
    {
      "text": "$\\texttt{MemoryRewardBench}$: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models",
      "url": "https://arxiv.org/abs/2601.11969"
    },
    {
      "text": "Distilling Time Series Foundation Models for Efficient Forecasting",
      "url": "https://arxiv.org/abs/2601.12785"
    }
  ],
  "lastUpdated": "2026-01-21T06:46:31Z"
}